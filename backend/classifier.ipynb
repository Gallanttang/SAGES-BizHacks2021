{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix, classification_report\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('amazon_au_marketing.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\n",
    "    'Dataset Origin', 'Match Rank', 'Match Score', \n",
    "    'Match Type', 'Upc', 'Product Available Inventory', \n",
    "    'Product Url', 'Retailer', 'Product Barcode', \n",
    "    'Product Model Number', 'Product Reviews Count', \n",
    "    'Bsr', 'Joining Key', 'Market', 'Product Currency',\n",
    "    'Sku', 'Uniq Id', 'Crawl Timestamp', 'Product Id',\n",
    "    'Product Company Type Source', 'Product Brand Normalised Source',\n",
    "    'Product Rating', 'Product Price'\n",
    "]\n",
    "\n",
    "df_cleaned = df.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_cleaned.columns)/len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Product Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['Product Category'].unique())\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df_cleaned, random_state=123)\n",
    "len(df_train)/len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'Home > Home DÃ©cor > Photo Frames'\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train.dropna(axis=0, subset=['Product Image Url'], inplace=True)\n",
    "df_test.dropna(axis=0, subset=['Product Image Url'], inplace=True)\n",
    "\n",
    "print(len(df_train)/len(df_test))\n",
    "\n",
    "# image_train = df_train['Product Image Url']\n",
    "x_train = df_train.drop(columns=['Product Category', 'Product Image Url'])\n",
    "y_train = df_train['Product Category'] == word\n",
    "\n",
    "# image_test = df_test['Product Image Url']\n",
    "x_test = df_test.drop(columns=['Product Category', 'Product Image Url'])\n",
    "y_test = df_test['Product Category'] == word\n",
    "\n",
    "x_train['corpus'] = x_train.apply(\n",
    "    lambda x: ','.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "x_train = x_train['corpus']\n",
    "\n",
    "x_test['corpus'] = x_test.apply(\n",
    "    lambda x: ','.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    ")\n",
    "x_test = x_test['corpus']\n",
    "\n",
    "del df_train\n",
    "del df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head().iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = DummyClassifier(strategy='prior')\n",
    "dc.fit(x_train, y_train)\n",
    "dc.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dc.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(dc, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_std(*args, **kwargs):\n",
    "    \"\"\"Like cross_validate, except also gives the standard deviation of the score\"\"\"\n",
    "    res = pd.DataFrame(cross_validate(*args, **kwargs))\n",
    "    res_mean = res.mean()\n",
    "\n",
    "    res_mean[\"std_test_score\"] = res[\"test_score\"].std()\n",
    "    if \"train_score\" in res:\n",
    "        res_mean[\"std_train_score\"] = res[\"train_score\"].std()\n",
    "    return res_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(countvec, LogisticRegression())\n",
    "\n",
    "cross_validate_std(pipe_lr, x_train, y_train, return_train_score=True, cv=5, scoring='precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe_lr.fit(x_train, y_train)\n",
    "plot_confusion_matrix(pipe_lr, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "171/(171+7+16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.DataFrame([sms.vector for sms in nlp.pipe(x_train)])\n",
    "x_test  = pd.DataFrame([sms.vector for sms in nlp.pipe(x_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate_std(lr, X_train_embeddings, y_train, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(lr, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.datasets import mnist\n",
    "# from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "# from tensorflow.keras.models import Sequential, Model, load_model\n",
    "# from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "# from tensorflow.keras import utils\n",
    "# from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "# from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
